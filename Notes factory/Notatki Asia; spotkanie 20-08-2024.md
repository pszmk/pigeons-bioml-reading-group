#### Co się działo
- na razie zostawiamy single-cell w spokoju
- KK robil target identification
	- z roznego rodzaju baz danych mozemy wyciagnac targety leków (czyli w co celuje lek, w jakie bialko). one nie sa idealne, ale są
	- jedno podejscie to wziecie istniejacych lekow i staramy sie przewidziec, co on targetuje
	- w ten sposob wyszukujemy nowe wskazania dla lekow
	- zwykle sie targety wymysla na poczatku, potem jest proces generacji molekuł
	- jedną z modalnosci jest gene expressions signatures
		- te sygnatury sa rozne, np L1000
		- my robimy perturbacje na lek, np wylacza sie gen i mierzy ekspresje przed i po
		- mozemy patrzec, ktore geny byly najbardziej narazone na zmiane (a nie wszystkie)
- jak sie generuje leki
	- smiles "notacja": jest to notacja atomów, wiązań itp (chemicznej struktury)
	- modeli jest bardzo duzo
	- conditional generation, nie dziala za dobrze, czesto bardzo zawezone warunkowanie (jak lek sie rozpuszcza w tluszczach)
	- rzadko sa modele, ktore sa conditionowane tym, ze np wywołują daną sygnaturę
	- fajnie jest miec model generatywny, ktory bierze pod uwage sygnature (czyli zmiane/wylaczenie genu; zmiane w ekspresji)
	- np w alzheimerze chcesz znalezc cos, co "odwraca" albo tworzy te sama sygnature jak chorobe albo target
	- miec molekule taką samą jak coś innego
- Co KK robil:
	- mial zrobic ogalna metode, ktora da sie zaaplikowac do tego konkretnego case'u
	- przykladowo: chcemy stworzyc generalny generatywny model (nieograniczona liczba warunkow i ich warunkow). uzywajac tego generujemy molekułę zgadzającą się z danymi
	- activity profiles
- mozna sie zainspirowac ESM-3. na inpucie tam wchodzi bialko
	- maskują część i iteracyjnie remaskują
	- istotne jest to, ze nie obserwujemy jakis danych
	- czyli "maskujemy" doslownie całą modalność
	- i chcemy przewidywac jak najwiecej
- Zbiory danych:
	- jak duze sa modele do generowania molekul?
	- jakies dane ze smiles'ami
	- dla 20000 lekow mamy zbiory danych z perturbacjami (ale miliony unlabelled)
	- jak 5000 lekow dziala na komorki rakowe  (sensitivities)
	- sporo danych z metadanymi
	- czyli duzo roznych datasetow
	- dla jednego warunku raczej da sie to zrobic
	- dla wielu moze byc problem
	- nie da sie ewaluowac tak wielu rzeczy
	- nigdy informacja nie jest pelna, ale jakas jest
- Pomysl:
	- spojrzec na warunki jak na zbior (slownik)
	- jeden model ktory encoduje smiles
	- drugi bierze i warunkuje na priorze (to moze byc wszystko). prior moze byc gaussian mixture, np mieszanka z 10 i funkcja jedynie zmienia wagi
	- mozna to przetestowac tak: mnist, wygenerowac funkcje  (grubosc/skosnosc)
	- wazne miec dostep do pelni wiedzy, mamy to maskowac
	- smiles sa mniejsze
	- zapewnic wlasnosci marginalizacji 
	- zaczac od baseline -> wybrac losowe property -> zaczac atakowac datasety z lekami
- Potencjalna komercjalizacja
	- bardzo blisko sa dane z peptydów