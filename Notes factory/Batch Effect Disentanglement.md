- see aae - they propose a simple mnist disentanglement setup
- below I propose a more complicated one
Basically extended batch conditioning. We may condition on the batch representation rather than the batch id to gain generalisation and attempt to learn the batch representation via contrastive learning to avoid specifying the classification heads although it shouldnt be too strong as different batches may be simmilar beacause of simmilar measurement conditions. Then their batch representations should be simmilar. Perhaps we could use as negative pair only the batches which are separated in the BE embedding space too much as negative pairs?
Whey we perturb the batch representation and use decoder (or pair of decoders if we are treating batch affect as residual) we want to optain the same biological signal and the same perturbed batch representation - this way we introduce a consistency regularization and prevent the batch representation from learning biological info hopefully - this leads to a kind of vae on the batch representation but with matching of the sampled representations in the latent not in the reconstruction - the reconstruction with some batch representation applied may be GAN compared with the samples using same batch representation if this batch representation is obtained from some other samples. Obviously we also want the samples from the same batch have the same batch representation which creates a simple anchoring in the latent. - *Should we really force the batch representation to be exactely the same for different samples from the same batch - we may have different granularities of batch definition (site, donor etc.)*
![[Pasted image 20240722084311.png]]